{
 "metadata": {
  "name": "",
  "signature": "sha256:6bbe55e87996507319ae357c076d360caeb3e4e7d614ce259d63eab78d691d1e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Audio analytics in Python final project part two"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Due date is December 19th at 10am**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Each person submits their own code and accompanying audio files**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**I'll be around at PierPont in the afternoon on Friday the 12th if anyone needs any help**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I wouldn't recommend using mpld3 in this instance as plotting long audio files using it can cause browser crashes due to the higher number of points"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.\tLoad in the urban audio recording 'brooklyn_street.wav'\n",
      "<br><br>\n",
      "2.\tPlot the waveform in the time domain with time in seconds on the x-axis\n",
      "<br><br>\n",
      "3.  Implement a moving average filter across the urban audio data. The frames should have these characteristics:\n",
      "    * A length of 125ms, but vary this to get the best result in your event detection algorithm\n",
      "    * They should overlap 50% with the previous frame\n",
      "    * Each frame should be windowed with a Hanning window\n",
      "    * Calculate the Root Mean Squared magnitude of each frame\n",
      "<br><br>\n",
      "4.  Plot this moving RMS array on the plot generated in part 2\n",
      "<br><br>\n",
      "5.  Implemement a peak/max amplitude tracking algorithm using the same parameters as used in Part 3, apply it to the urban audio waveform and plot this on the same time domain plot from Part 2\n",
      "<br><br>\n",
      "6.  Generate a spectrogram of this audio data with time in seconds on the x-axis, frequency on the y-axis and color representing amplitude. Plot this below a plot of the signal in the time domain, ensuring that the x-axis lines up/corresponds in time\n",
      "<br><br>\n",
      "7.  Implement an event detection algorithm that will automatically identify salient sound sources within the urban audio recording. Salient sound sources mean sounds that stand out from the background noise as distinct sonic events.\n",
      "    * The algorithm can use a combination of time domain and frequency domain techniques\n",
      "    * The algorithm should utilise a moving window approach\n",
      "    * Add individually colored regions (<1.0 alpha) to a time domain and frequency domain (spectrogram) plots identifying the start and end time of each sound event\n",
      "    * Output start time, end time, duration, amplitude (RMS) and at least two spectral features for each identified event\n",
      "    * Save each identified sound event as a wave file, named by its start time in milliseconds\n",
      "    * There are around 10 salient events in the urban audio recording"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}